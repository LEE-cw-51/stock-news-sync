import os
import logging
from groq import Groq
from dotenv import load_dotenv

load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
_groq_api_key = os.getenv("GROQ_API_KEY")
client = Groq(api_key=_groq_api_key) if _groq_api_key else None

# [í™•ì¥ì„±] 2026ë…„ ê¸°ì¤€ ê°€ìš© ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
# 1ìˆœìœ„ ëª¨ë¸ ì‹¤íŒ¨ ì‹œ 2ìˆœìœ„ë¡œ ìë™ ì „í™˜ë©ë‹ˆë‹¤.
MODEL_FALLBACK_LIST = [
    "llama-3.1-8b-instant",  # 1ìˆœìœ„: ì´ˆê³ ì†, íš¨ìœ¨ì  ìš”ì•½
    "openai/gpt-oss-20b"     # 2ìˆœìœ„: ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ ì»¤ì„œ ë” ì •êµí•œ ë¶„ì„ ê°€ëŠ¥
]

def generate_ai_summary(stock_name, context):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ëª¨ë¸ í´ë°±(Fallback) ê¸°ëŠ¥ì„ í†µí•´ API í•œë„ ë° ì„œë¹„ìŠ¤ ì¤‘ë‹¨ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
    """
    if not client:
        logger.warning("GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return "AI ì„œë¹„ìŠ¤ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if not context:
        return "ìµœê·¼ 24ì‹œê°„ ë‚´ ê´€ë ¨ëœ ì¤‘ìš” ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    system_prompt = "ë‹¹ì‹ ì€ ëƒ‰ì² í•œ íŒ©íŠ¸ ê¸°ë°˜ì˜ ì£¼ì‹ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤."
    
    # Input Tokenì„ ì•„ë¼ê¸° ìœ„í•´ ëª…í™•í•œ ì¶œë ¥ ì–‘ì‹ ì§€ì •
    user_prompt = f"""
    [ë‰´ìŠ¤ ë°ì´í„°]
    {context}

    [ì„ë¬´]
    ìœ„ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ '{stock_name}'ì— ëŒ€í•œ íˆ¬ìììš© ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.
    
    [ì¶œë ¥ ì–‘ì‹]
    1. ğŸ” **í•µì‹¬ ìš”ì•½**: ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆ 3ê°€ì§€ë¥¼ ë¶ˆë ›í¬ì¸íŠ¸ë¡œ ìš”ì•½ (í•œêµ­ì–´).
    2. ğŸ“Š **ì‹œì¥ ë°˜ì‘**: ë‰´ìŠ¤ê°€ ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥(í˜¸ì¬/ì•…ì¬/ì¤‘ë¦½)ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ.
    """

    for model_name in MODEL_FALLBACK_LIST:
        try:
            logger.info(f"ğŸ¤– AI ë¶„ì„ ì‹œë„ ì¤‘... (ëª¨ë¸: {model_name})")
            
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2, # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                max_tokens=500,  # Output Token í•œë„ ê´€ë¦¬
            )
            
            result = completion.choices[0].message.content
            logger.info(f"âœ… AI ë¶„ì„ ì™„ë£Œ (ëª¨ë¸: {model_name})")
            return result

        except Exception as e:
            # 429(í•œë„ì´ˆê³¼) ë˜ëŠ” 400(ëª¨ë¸ ë§Œë£Œ) ë“±ì˜ ì—ëŸ¬ ì‹œ ë‹¤ìŒ ëª¨ë¸ ì‹œë„
            error_msg = str(e)
            if "429" in error_msg or "model_decommissioned" in error_msg or "400" in error_msg:
                logger.warning(f"âš ï¸ {model_name} ì‚¬ìš© ë¶ˆê°€ ë˜ëŠ” í•œë„ ì´ˆê³¼. ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                continue 
            else:
                logger.error(f"âŒ {model_name} ì˜ˆì™¸ ë°œìƒ: {e}")
                continue

    return "í˜„ì¬ ëª¨ë“  AI ëª¨ë¸ì˜ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆê±°ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."