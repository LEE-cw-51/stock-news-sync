import os
import logging
from openai import OpenAI, RateLimitError  # [P5 Fix] RateLimitError íƒ€ì… ì„í¬íŠ¸
from dotenv import load_dotenv

try:
    from backend.config.models import MODEL_CONFIG, MAX_TOKENS, TEMPERATURE
except ModuleNotFoundError:
    from config.models import MODEL_CONFIG, MAX_TOKENS, TEMPERATURE

load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# Groqì™€ Gemini ëª¨ë‘ OpenAI API í˜•ì‹ì„ ì§€ì›í•˜ë¯€ë¡œ base_urlë§Œ ë‹¬ë¦¬í•´ í†µí•©í•©ë‹ˆë‹¤.
# =============================================================================
_GROQ_CLIENT = OpenAI(
    api_key=os.getenv("GROQ_API_KEY", ""),
    base_url="https://api.groq.com/openai/v1",
) if os.getenv("GROQ_API_KEY") else None

_GEMINI_CLIENT = OpenAI(
    api_key=os.getenv("GEMINI_API_KEY", ""),
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
) if os.getenv("GEMINI_API_KEY") else None

if not _GROQ_CLIENT:
    logger.error("âŒ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not _GEMINI_CLIENT:
    logger.error("âŒ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Lambda ì‹¤í–‰ ë‚´ 429 ì´ˆê³¼ ëª¨ë¸ì„ ê¸°ì–µ â†’ ê°™ì€ ì„¸ì…˜ì—ì„œ ì¬ì‹œë„ ë°©ì§€
_quota_exceeded_models: set = set()


def _get_client_and_model(model_name: str):
    """
    ëª¨ë¸ ì´ë¦„ì˜ prefixë¡œ í´ë¼ì´ì–¸íŠ¸ì™€ ì‹¤ì œ API ëª¨ë¸ëª…ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
      "groq/<model>"   â†’ _GROQ_CLIENT   + "<model>"
      "gemini/<model>" â†’ _GEMINI_CLIENT + "<model>"
    """
    if model_name.startswith("groq/"):
        if not _GROQ_CLIENT:
            raise Exception("GROQ_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
        return _GROQ_CLIENT, model_name[len("groq/"):]

    if model_name.startswith("gemini/"):
        if not _GEMINI_CLIENT:
            raise Exception("GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
        return _GEMINI_CLIENT, model_name[len("gemini/"):]

    raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ prefix: {model_name}")


def generate_ai_summary(stock_name: str, context: str, category: str = "watchlist") -> str:
    """
    ì¹´í…Œê³ ë¦¬ë³„ ìµœì  ëª¨ë¸ë¡œ AI ë¸Œë¦¬í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ëª¨ë¸ ìš°ì„ ìˆœìœ„: backend/config/models.py ì—ì„œ ì„¤ì •
    - category: "macro" | "portfolio" | "watchlist"
    """
    if not context:
        return "ìµœê·¼ 24ì‹œê°„ ë‚´ ê´€ë ¨ëœ ì¤‘ìš” ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # í™˜ê° ë°©ì§€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = """ë‹¹ì‹ ì€ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ì˜ ì‹œë‹ˆì–´ ì£¼ì‹ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ìœ ì¼í•œ ëª©í‘œëŠ” ì œê³µëœ [ë‰´ìŠ¤ ì›ë¬¸]ì—ì„œ 'íŒ©íŠ¸'ì™€ 'ìˆ˜ì¹˜'ë§Œì„ ì¶”ì¶œí•˜ì—¬ íˆ¬ììì—ê²Œ ê°ê´€ì ì¸ ë¸Œë¦¬í•‘ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
[ì ˆëŒ€ ê·œì¹™ - ìœ„ë°˜ ì‹œ í˜ë„í‹°]
1. í™˜ê° ê¸ˆì§€: ì œê³µëœ [ë‰´ìŠ¤ ì›ë¬¸]ì— ì—†ëŠ” ì •ë³´ë‚˜ ê³¼ê±° ì§€ì‹ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤.
2. ìˆ˜ì¹˜ ìš°ì„ : ì£¼ê°€, ëª©í‘œê°€, ì‹¤ì  í¼ì„¼íŠ¸(%), ë‚ ì§œ ë“± ìˆ«ìê°€ í¬í•¨ëœ ë¬¸ë§¥ì€ ë°˜ë“œì‹œ ìš”ì•½ì— í¬í•¨í•˜ì‹­ì‹œì˜¤.
3. ê°„ê²°ì„±: ë¯¸ì‚¬ì—¬êµ¬ë¥¼ ë¹¼ê³  ê±´ì¡°í•˜ê³  ì „ë¬¸ì ì¸ ë¦¬í¬íŠ¸ í†¤(ê°œì¡°ì‹)ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
4. ì–¸ì–´: ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."""

    user_prompt = f"""
    [ë¶„ì„ ëŒ€ìƒ ì¢…ëª©]: {stock_name}

    [ë‰´ìŠ¤ ë°ì´í„°]
    {context}
    [ì„ë¬´]
    ìœ„ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ '{stock_name}'ì— ëŒ€í•œ íˆ¬ìììš© ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

    [ì¶œë ¥ ì–‘ì‹]
    1. ğŸ” **í•µì‹¬ ìš”ì•½**: ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆ 3ê°€ì§€ë¥¼ ë¶ˆë ›í¬ì¸íŠ¸ë¡œ ìš”ì•½.
    2. ğŸ“Š **ì‹œì¥ ë°˜ì‘ ì˜ˆìƒ**: (ë‹¨ê¸°ì  ê´€ì ì—ì„œ ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ì„ 'í˜¸ì¬', 'ì•…ì¬', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ë¡œ ëª…ì‹œí•˜ê³  ê·¸ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ )
    """

    models = MODEL_CONFIG.get(category, MODEL_CONFIG["watchlist"])

    for model_name in models:
        # ì´ë²ˆ Lambda ì‹¤í–‰ì—ì„œ ì´ë¯¸ 429ê°€ ë°œìƒí•œ ëª¨ë¸ì€ ì¦‰ì‹œ ê±´ë„ˆëœ€
        if model_name in _quota_exceeded_models:
            logger.info(f"â­ï¸ {model_name} í• ë‹¹ëŸ‰ ì´ˆê³¼ ì´ë ¥ - ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        try:
            client, api_model = _get_client_and_model(model_name)
            logger.info(f"ğŸ¤– [{category.upper()}] AI ë¶„ì„ ì‹œë„ ì¤‘... (ëª¨ë¸: {model_name})")

            response = client.chat.completions.create(
                model=api_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
            )

            # í† í° ì œí•œìœ¼ë¡œ ì¶œë ¥ì´ ì˜ë¦° ê²½ìš° â†’ ë‹¤ìŒ ëª¨ë¸ë¡œ í´ë°±
            if response.choices[0].finish_reason == "length":
                raise Exception("ì¶œë ¥ì´ í† í° ì œí•œìœ¼ë¡œ ì˜ë¦¼ - ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜")

            result = response.choices[0].message.content
            logger.info(f"âœ… AI ë¶„ì„ ì™„ë£Œ (ëª¨ë¸: {model_name})")
            return result

        except RateLimitError:
            # [P5 Fix] openai SDKì˜ RateLimitError(HTTP 429)ë¥¼ íƒ€ì…ìœ¼ë¡œ ì •í™•íˆ ê°ì§€
            _quota_exceeded_models.add(model_name)
            logger.warning(f"âš ï¸ {model_name} í• ë‹¹ëŸ‰ ì´ˆê³¼(429) - ì„¸ì…˜ ë¹„í™œì„±í™” ë° ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            continue

        except Exception as e:
            error_str = str(e)
            # [P5 Fix] Geminiì˜ RESOURCE_EXHAUSTEDëŠ” openai RateLimitErrorê°€ ì•„ë‹Œ
            # ì¼ë°˜ Exceptionìœ¼ë¡œ ë˜í•‘ë  ìˆ˜ ìˆì–´ ë¬¸ìì—´ ì²´í¬ë¥¼ í´ë°±ìœ¼ë¡œ ìœ ì§€
            if "RESOURCE_EXHAUSTED" in error_str or "429" in error_str:
                _quota_exceeded_models.add(model_name)
                logger.warning(f"âš ï¸ {model_name} í• ë‹¹ëŸ‰ ì´ˆê³¼(429) - ì„¸ì…˜ ë¹„í™œì„±í™” ë° ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            else:
                logger.warning(f"âš ï¸ {model_name} ì‹¤íŒ¨ ({error_str}) -> ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            continue

    return "í˜„ì¬ ëª¨ë“  AI ëª¨ë¸ì˜ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆê±°ë‚˜ ì‘ë‹µí•  ìˆ˜ ì—†ëŠ” ìƒíƒœì…ë‹ˆë‹¤."
